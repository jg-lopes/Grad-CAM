{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzN8JYeV0X4b"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we will introduce the Grad-CAM which visualizes the heatmap of input images by highlighting the important region for visual question answering(VQA) task.\n",
    "\n",
    "* **To be submitted**: this notebook in two weeks, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY_Ih5Fr0X4h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82veFW30X4i"
   },
   "source": [
    "### Visual Question Answering problem\n",
    "Given an image and a question in natural language, the model choose the most likely answer from 3 000 classes according to the content of image. The VQA task is indeed a multi-classificaition problem.\n",
    "<img src=\"vqa_model.PNG\">\n",
    "\n",
    "We provide you a pretrained model `vqa_resnet` for VQA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caL_asOy0X4j"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "from load_model import load_model\n",
    "vqa_resnet = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UopJ78UT0X4j",
    "outputId": "7f41348e-bc67-499a-b238-ba8fe8721ee2"
   },
   "outputs": [],
   "source": [
    "print(vqa_resnet) # for more information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1cFOaQQ0X4j"
   },
   "outputs": [],
   "source": [
    "checkpoint = '2017-08-04_00.55.19.pth'\n",
    "saved_state = torch.load(checkpoint, map_location=device)\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state['vocab']\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab['question']\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab['answer']\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = ['unk'] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx]=w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvqTHpqo0X4j"
   },
   "source": [
    "### Inputs\n",
    "In order to use the pretrained model, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(448, 448)`. You can call the function `image_to_features` to achieve image preprocessing. For input question, the function `encode_question` is provided to encode the question into a vector of indices. You can also use `preprocess` function for both image and question preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKuCGGwD0X4k"
   },
   "outputs": [],
   "source": [
    "def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(int(target_size / central_fraction)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzFKIzy70X4k"
   },
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    question_arr = question.lower().split()\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2shnMBZU0X4l"
   },
   "outputs": [],
   "source": [
    "# preprocess requires the dir_path of an image and the associated question. \n",
    "#It returns the spectific input form which can be used directly by vqa model. \n",
    "def preprocess(dir_path, question):\n",
    "    q, q_len = encode_question(question)\n",
    "    img = Image.open(dir_path).convert('RGB')\n",
    "    image_size = 448  # scale image to given size and center\n",
    "    central_fraction = 1.0\n",
    "    transform = get_transform(image_size, central_fraction=central_fraction)\n",
    "    img_transformed = transform(img)\n",
    "    img_features = img_transformed.unsqueeze(0).to(device)\n",
    "    \n",
    "    inputs = (img_features, q.unsqueeze(0), q_len.unsqueeze(0))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYGqe0WR0X4l"
   },
   "source": [
    "We provide you two pictures and some question-answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "hbAiW1MI0X4m",
    "outputId": "ce2e50c8-2caf-45db-a268-69ee5ee2d068"
   },
   "outputs": [],
   "source": [
    "Question1 = 'What animal'\n",
    "Answer1 = ['dog','cat' ]\n",
    "indices1 = [answer_to_index[ans] for ans in Answer1]# The indices of category \n",
    "img1 = Image.open('dog_cat.png')\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moL4Kbq70X4m",
    "outputId": "adcbe3da-606f-4c7d-f82d-214019d6951c"
   },
   "outputs": [],
   "source": [
    "dir_path = 'dog_cat.png' \n",
    "inputs = preprocess(dir_path, Question1)\n",
    "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
    "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
    "print(answer_words[answer_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "dw3zDM7f0X4n",
    "outputId": "40d4a275-b2fa-404e-f771-247b30fe6b24"
   },
   "outputs": [],
   "source": [
    "Question2 = 'What color'\n",
    "Answer2 = ['green','yellow' ]\n",
    "indices2 = [answer_to_index[ans] for ans in Answer2]\n",
    "img2 = Image.open('hydrant.png')\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpvvKlVJ0X4n",
    "outputId": "b020b448-7387-4018-c301-c2067bce6647"
   },
   "outputs": [],
   "source": [
    "dir_path = 'hydrant.png' \n",
    "inputs = preprocess(dir_path, Question2)\n",
    "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
    "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
    "print(answer_words[answer_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7i1blb90X4n"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image with a question, and a category (‘dog’) as input, we foward propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (dog), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the two images. For each image, consider the answers we provided as the desired classes. Compare the heatmaps of different answers, and conclude. \n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model `vqa_resnet` doesn't have the activation function after its last layer, the output is indeed the `raw class scores`, you can use it directly. Run \"print(vqa_resnet)\" to get more information on VGG model.\n",
    " + The last CNN layer of the model is: `vqa_resnet.resnet_layer4.r_model.layer4[2].conv3` \n",
    " + The size of feature maps is 14x14, so as your heatmap. You need to project the heatmap to the original image(224x224) to have a better observation. The function `cv2.resize()` may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrfBZo9W0X4o"
   },
   "source": [
    "<img src=\"./grad_cam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_aFLlL7EeWj"
   },
   "source": [
    "A gradCAM function is introduced that orchestrates all of the behavior necessary to take the desired inputs, extract the gradients, generate the heatmap and, finally, output an image into Jupyter with the final result.\n",
    "\n",
    "This function takes the question and file path of the source image as parameters, as well as the desired class index upon which the visualization will be based upon.\n",
    "\n",
    "The flow of the function is as follows:\n",
    "\n",
    "1. It defines gradCAMHook, a hook function that deals with the gradient extraction, heatmap generation and image output\n",
    "2. It attaches this function with a hook to the final layer of the trained network\n",
    "3. It foward feeds the question to the source image\n",
    "4. It backpropagates using the class index\n",
    "5. (Implicit) The hook is called during backpropagation, initiating the heatmap generation and the output of the Grad-CAM image.\n",
    "6. The Hook is removed from the network in order to return the network to its original status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1huokuM0zgLu"
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vudIT9CNtBK_"
   },
   "outputs": [],
   "source": [
    "def gradCAM(question, source_image, class_index):\n",
    "\n",
    "    # GRADCAM STEP\n",
    "    hk_output = None\n",
    "    hk_grad_output = None\n",
    "\n",
    "    # Setting up the hooks        \n",
    "    def frontHook(self, input, output):\n",
    "        # nonlocal makes so the variable writes on the gradCAM function scope\n",
    "        nonlocal hk_output\n",
    "        # Detaching to avoid requires_grad\n",
    "        hk_output = output.detach()\n",
    "\n",
    "    def backHook(self, grad_input, grad_output):\n",
    "        nonlocal hk_grad_output\n",
    "        hk_grad_output = grad_output[0].detach()\n",
    "\n",
    "    fr_hook = vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_forward_hook(frontHook)\n",
    "    bk_hook = vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_backward_hook(backHook)\n",
    "    \n",
    "    # Creating the query for the model\n",
    "    inputs = preprocess(source_image, question)\n",
    "\n",
    "    # Foward feeding the inputs -> will call the attached hook\n",
    "    out = vqa_resnet(*inputs)\n",
    "\n",
    "    # Backpropagating the output -> will call the attached hook\n",
    "    vqa_resnet.zero_grad()\n",
    "    out[:, class_index].backward()\n",
    "\n",
    "    # From this point foward, the hk_* variables are set\n",
    "    # The hooks are no longer necessary so we should remove them\n",
    "    bk_hook.remove()\n",
    "    fr_hook.remove()\n",
    "    \n",
    "    # Setting up equation (1) from pg. 4 of the paper\n",
    "    weights = torch.mean(hk_grad_output, dim=[0, 2, 3])\n",
    "\n",
    "    \n",
    "    # Multiplying the activation (output) by the new weights\n",
    "    for i in range(hk_grad_output.shape[1]):\n",
    "        hk_output[:, i, :, :] *= weights[i]\n",
    "\n",
    "    # Summing up as per equation (2) from pg. 5 of the paper\n",
    "    gc_result = torch.mean(hk_output, dim=1).squeeze()\n",
    "\n",
    "    # Relu wrapping up equation (2)\n",
    "    gc_result = np.maximum(gc_result, 0)\n",
    "\n",
    "    # Resizing the matrix from 14x14 to the image resolution 224x224\n",
    "    heatmap = cv2.resize(gc_result.numpy(), (224, 224))\n",
    "\n",
    "    # PRODUCING THE HEATMAP\n",
    "    \n",
    "    # Normalizing the heatmap\n",
    "    heatmap = cv2.normalize(heatmap, heatmap, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # Applying a colormap to heatmap\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * (255 - heatmap)), cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "    # Outputting final result\n",
    "    # Loading the background image\n",
    "    source_img = cv2.imread(source_image)\n",
    "\n",
    "    # Overlaying both images\n",
    "    final_image = heatmap * 0.5 + source_img * 0.5\n",
    "\n",
    "    # A glitch in cv2.imshow crashes Jupyter\n",
    "    # Converting so the image can be displayed using plt.imshow\n",
    "    final_image = cv2.cvtColor(final_image.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEc-0atLyCc3",
    "outputId": "7e65f870-e5c8-461e-c80d-8a48d7db09b8"
   },
   "outputs": [],
   "source": [
    "# Recall originally for image 1\n",
    "print(Answer1)\n",
    "print(indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "7FYDFss_yCc7",
    "outputId": "a7e21737-d8c7-4f11-9405-ab64ffc4d77f"
   },
   "outputs": [],
   "source": [
    "question = 'What animal'\n",
    "source_image = 'dog_cat.png'\n",
    "class_index = answer_to_index['dog']\n",
    "\n",
    "gradCAM(question, source_image, class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "jf7jgNWmyCc7",
    "outputId": "57d88d4f-69c5-436b-aa84-972ab2b87201"
   },
   "outputs": [],
   "source": [
    "question = 'What animal'\n",
    "source_image = 'dog_cat.png'\n",
    "class_index = answer_to_index['cat']\n",
    "\n",
    "gradCAM(question, source_image, class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJOpZ0WBFwRS"
   },
   "source": [
    "On this example, we can clearly see that Grad-CAM correctly identifies both classes of cat and dog, focusing on their most distinctive characteristics, their faces.\n",
    "\n",
    "Even if two actors are present in the image, the given class is enough for it to concern itself with only the desired agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6Y3yUoqyCc7",
    "outputId": "780edd3e-324b-44fd-c632-f520a278cae6"
   },
   "outputs": [],
   "source": [
    "print(Answer2)\n",
    "print(indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "HX0iYyJwyCc8",
    "outputId": "d1f0da52-2ab4-41f8-b99e-195535d5e4af"
   },
   "outputs": [],
   "source": [
    "question = 'What color'\n",
    "source_image = 'hydrant.png'\n",
    "class_index = answer_to_index['green']\n",
    "\n",
    "gradCAM(question, source_image, class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "O1ZxFWC2yCc8",
    "outputId": "5436797f-241e-4adb-999d-419036008ac7"
   },
   "outputs": [],
   "source": [
    "question = 'What color'\n",
    "source_image = 'hydrant.png'\n",
    "class_index = answer_to_index['yellow']\n",
    "\n",
    "gradCAM(question, source_image, class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afJzHE55GWPk"
   },
   "source": [
    "On this example, we see that while Grad-CAM was able to correctly detect the interest into the green hydrant, a mixed result was found for the yellow one.\n",
    "\n",
    "When querying for yellow, the results focuses both on yellow and green, with a higher focus on the green one. While it is difficult to visualize for sure the exact reason for this behavior, two theories can be hypothesized:\n",
    "\n",
    "1. The green hydrant is closer, making it bigger on the overall image. Since the original output was in 14x14 format, this could make the yellow hydrant too small on the convolution, while the green one just big enough to be noticed.\n",
    "\n",
    "2. The reason why the green one would be noticed is that in RGB yellow is classified as (255, 255, 0). That is, yellow contains a full channel of green.\n",
    "\n",
    "Both these theories in conjunction might explain, for example, why the blue hydrant wasn't been chosen (blue channel is 0 for yellow) and the red hydrant was not chosen as well (it is too far back in the image, probably causing issues in the 14x14 representation).\n",
    "\n",
    "Overall, even with this small inconsistency, it is still very successful in detecting the color and the point of interest in this rather complex image."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Grad_CAM_Joao_Gabriel_LOPES_DE_OLIVEIRA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
